#!/usr/bin/env python3
"""
Frequency Analysis for TimesFM Optimization
===========================================

This script analyzes the frequency characteristics of our ES (E-mini S&P 500) dataset
to determine the optimal frequency setting for TimesFM predictions.

We'll analyze:
1. Intraday volatility patterns
2. Swing trading cycles (15min, 30min, 1hr, 4hr patterns)
3. Autocorrelation at different lags
4. Spectral density analysis
5. Optimal prediction horizons for different timeframes

Author: AI Assistant
Date: 2025-08-26
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import random
from scipy import signal
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# Configuration
# ==============================================================================

# Data paths
DATASETS_DIR = Path("datasets/ES")
OUTPUT_DIR = Path("frequency_analysis_plots")
OUTPUT_DIR.mkdir(exist_ok=True)

# Analysis parameters
SAMPLE_SIZE = 10  # Number of random days to analyze
MIN_DATA_POINTS = 1000  # Minimum data points per day
CONTEXT_LENGTH = 416  # Current TimesFM context length
HORIZON_LENGTH = 96   # Current prediction horizon

# Frequency bands for analysis (in minutes)
FREQUENCY_BANDS = {
    'Ultra High Freq': 1,      # 1-minute (scalping)
    'High Freq': 5,            # 5-minute (day trading)
    'Medium Freq': 15,         # 15-minute (swing intraday)
    'Low Freq': 60,            # 1-hour (swing trading)
    'Very Low Freq': 240       # 4-hour (position trading)
}

# ==============================================================================
# Data Loading Functions
# ==============================================================================

def load_random_es_files(n_files=SAMPLE_SIZE):
    """Load random ES dataset files for analysis."""
    print(f"üìÅ Loading {n_files} random ES dataset files...")
    
    # Get all CSV files
    csv_files = list(DATASETS_DIR.glob("*.csv"))
    if len(csv_files) == 0:
        raise FileNotFoundError(f"No CSV files found in {DATASETS_DIR}")
    
    # Select random files
    selected_files = random.sample(csv_files, min(n_files, len(csv_files)))
    
    datasets = []
    for file_path in selected_files:
        try:
            # Load the dataset
            df = pd.read_csv(file_path)
            
            # Ensure we have required columns
            required_cols = ['timestamp_pt', 'open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_cols):
                print(f"‚ö†Ô∏è  Skipping {file_path.name}: Missing required columns")
                print(f"     Available columns: {list(df.columns)}")
                continue
            
            # Filter for sufficient data
            if len(df) < MIN_DATA_POINTS:
                print(f"‚ö†Ô∏è  Skipping {file_path.name}: Insufficient data ({len(df)} points)")
                continue
            
            # Parse timestamp
            df['timestamp'] = pd.to_datetime(df['timestamp_pt'])
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            # Add metadata
            df['date'] = df['timestamp'].dt.date
            df['file_name'] = file_path.stem
            
            datasets.append(df)
            print(f"‚úÖ Loaded {file_path.name}: {len(df)} data points")
            
        except Exception as e:
            print(f"‚ùå Error loading {file_path.name}: {e}")
            continue
    
    print(f"üìä Successfully loaded {len(datasets)} datasets")
    return datasets

def normalize_price_data(prices):
    """Normalize price data relative to first price."""
    if len(prices) == 0:
        return np.array([])
    if hasattr(prices, 'iloc'):
        # pandas Series
        return prices / prices.iloc[0]
    else:
        # numpy array
        return prices / prices[0]

# ==============================================================================
# Frequency Analysis Functions
# ==============================================================================

def analyze_intraday_volatility(datasets):
    """Analyze intraday volatility patterns."""
    print("\nüìà Analyzing intraday volatility patterns...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Intraday Volatility Analysis', fontsize=16, fontweight='bold')
    
    all_hourly_vol = []
    all_returns = []
    
    for dataset in datasets:
        df = dataset.copy()
        
        # Calculate returns
        df['returns'] = df['close'].pct_change()
        df['abs_returns'] = df['returns'].abs()
        
        # Add hour information
        df['hour'] = df['timestamp'].dt.hour
        df['minute'] = df['timestamp'].dt.minute
        
        # Hourly volatility
        hourly_vol = df.groupby('hour')['abs_returns'].mean()
        all_hourly_vol.append(hourly_vol)
        all_returns.extend(df['returns'].dropna().values)
    
    # Average hourly volatility
    avg_hourly_vol = pd.concat(all_hourly_vol, axis=1).mean(axis=1)
    
    # Plot 1: Hourly volatility pattern
    axes[0, 0].plot(avg_hourly_vol.index, avg_hourly_vol.values, 'b-', linewidth=2, marker='o')
    axes[0, 0].set_title('Average Hourly Volatility')
    axes[0, 0].set_xlabel('Hour of Day')
    axes[0, 0].set_ylabel('Average Absolute Returns')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Plot 2: Returns distribution
    axes[0, 1].hist(all_returns, bins=100, alpha=0.7, density=True)
    axes[0, 1].set_title('Returns Distribution')
    axes[0, 1].set_xlabel('Returns')
    axes[0, 1].set_ylabel('Density')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Plot 3: Volatility clustering analysis
    sample_df = datasets[0].copy()
    sample_df['returns'] = sample_df['close'].pct_change()
    sample_df['rolling_vol'] = sample_df['returns'].rolling(window=20).std()
    
    axes[1, 0].plot(sample_df.index[:500], sample_df['rolling_vol'].iloc[:500], 'r-', alpha=0.7)
    axes[1, 0].set_title('Volatility Clustering (Sample Day)')
    axes[1, 0].set_xlabel('Time Index')
    axes[1, 0].set_ylabel('Rolling Volatility (20-period)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Plot 4: Market session analysis
    session_vol = {}
    for dataset in datasets:
        df = dataset.copy()
        df['returns'] = df['close'].pct_change()
        df['abs_returns'] = df['returns'].abs()
        df['hour'] = df['timestamp'].dt.hour
        
        # Define market sessions (EST)
        pre_market = df[df['hour'].between(4, 9)]['abs_returns'].mean()
        regular_hours = df[df['hour'].between(9, 16)]['abs_returns'].mean()
        after_hours = df[df['hour'].between(16, 20)]['abs_returns'].mean()
        
        session_vol[df['file_name'].iloc[0]] = {
            'Pre-Market': pre_market,
            'Regular Hours': regular_hours,
            'After Hours': after_hours
        }
    
    session_df = pd.DataFrame(session_vol).T
    session_means = session_df.mean()
    
    axes[1, 1].bar(session_means.index, session_means.values, alpha=0.7)
    axes[1, 1].set_title('Average Volatility by Market Session')
    axes[1, 1].set_ylabel('Average Absolute Returns')
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'intraday_volatility_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return avg_hourly_vol, session_means

def analyze_autocorrelation(datasets, max_lag=100):
    """Analyze autocorrelation patterns at different lags."""
    print(f"\nüîÑ Analyzing autocorrelation patterns (max lag: {max_lag})...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Autocorrelation Analysis', fontsize=16, fontweight='bold')
    
    all_autocorr_close = []
    all_autocorr_returns = []
    all_autocorr_vol = []
    
    for dataset in datasets:
        df = dataset.copy()
        
        # Normalize close prices
        normalized_close = normalize_price_data(df['close'])
        
        # Calculate returns and volatility
        returns = df['close'].pct_change().dropna()
        volatility = returns.rolling(window=20).std().dropna()
        
        # Calculate autocorrelations
        if len(normalized_close) > max_lag:
            autocorr_close = [np.corrcoef(normalized_close[:-i], normalized_close[i:])[0, 1] 
                             for i in range(1, min(max_lag + 1, len(normalized_close)))]
            all_autocorr_close.append(autocorr_close)
        
        if len(returns) > max_lag:
            autocorr_returns = [np.corrcoef(returns[:-i], returns[i:])[0, 1] 
                               for i in range(1, min(max_lag + 1, len(returns)))]
            all_autocorr_returns.append(autocorr_returns)
        
        if len(volatility) > max_lag:
            autocorr_vol = [np.corrcoef(volatility[:-i], volatility[i:])[0, 1] 
                           for i in range(1, min(max_lag + 1, len(volatility)))]
            all_autocorr_vol.append(autocorr_vol)
    
    # Average autocorrelations
    if all_autocorr_close:
        avg_autocorr_close = np.mean(all_autocorr_close, axis=0)
        lags = range(1, len(avg_autocorr_close) + 1)
        
        axes[0, 0].plot(lags, avg_autocorr_close, 'b-', linewidth=2)
        axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        axes[0, 0].set_title('Autocorrelation: Normalized Close Prices')
        axes[0, 0].set_xlabel('Lag (minutes)')
        axes[0, 0].set_ylabel('Autocorrelation')
        axes[0, 0].grid(True, alpha=0.3)
    
    if all_autocorr_returns:
        avg_autocorr_returns = np.mean(all_autocorr_returns, axis=0)
        lags = range(1, len(avg_autocorr_returns) + 1)
        
        axes[0, 1].plot(lags, avg_autocorr_returns, 'r-', linewidth=2)
        axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        axes[0, 1].set_title('Autocorrelation: Returns')
        axes[0, 1].set_xlabel('Lag (minutes)')
        axes[0, 1].set_ylabel('Autocorrelation')
        axes[0, 1].grid(True, alpha=0.3)
    
    if all_autocorr_vol:
        avg_autocorr_vol = np.mean(all_autocorr_vol, axis=0)
        lags = range(1, len(avg_autocorr_vol) + 1)
        
        axes[1, 0].plot(lags, avg_autocorr_vol, 'g-', linewidth=2)
        axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        axes[1, 0].set_title('Autocorrelation: Volatility')
        axes[1, 0].set_xlabel('Lag (minutes)')
        axes[1, 0].set_ylabel('Autocorrelation')
        axes[1, 0].grid(True, alpha=0.3)
    
    # Partial autocorrelation for close prices
    if all_autocorr_close:
        # Simple partial autocorr approximation
        partial_autocorr = []
        for lag in range(1, min(21, len(avg_autocorr_close))):
            if lag == 1:
                partial_autocorr.append(avg_autocorr_close[0])
            else:
                # Simplified calculation
                partial_autocorr.append(avg_autocorr_close[lag-1] * 0.9)  # Approximation
        
        axes[1, 1].bar(range(1, len(partial_autocorr) + 1), partial_autocorr, alpha=0.7)
        axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        axes[1, 1].set_title('Partial Autocorrelation: Close Prices (Approx)')
        axes[1, 1].set_xlabel('Lag (minutes)')
        axes[1, 1].set_ylabel('Partial Autocorrelation')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'autocorrelation_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return avg_autocorr_close if all_autocorr_close else None

def analyze_spectral_density(datasets):
    """Analyze spectral density to identify dominant frequencies."""
    print("\nüåä Analyzing spectral density...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Spectral Density Analysis', fontsize=16, fontweight='bold')
    
    all_psds = []
    all_freqs = []
    
    for dataset in datasets[:3]:  # Analyze first 3 datasets for performance
        df = dataset.copy()
        
        # Normalize close prices
        normalized_close = normalize_price_data(df['close'])
        
        # Calculate returns
        returns = df['close'].pct_change().dropna()
        
        # Spectral analysis for close prices
        if len(normalized_close) > 100:
            freqs, psd = signal.periodogram(normalized_close, fs=1.0)  # 1 sample per minute
            all_psds.append(psd)
            all_freqs.append(freqs)
            
            # Plot individual PSD
            axes[0, 0].loglog(freqs[1:], psd[1:], alpha=0.5, label=f"{df['file_name'].iloc[0]}")
    
    if all_psds:
        # Average PSD
        min_len = min(len(psd) for psd in all_psds)
        avg_psd = np.mean([psd[:min_len] for psd in all_psds], axis=0)
        avg_freqs = all_freqs[0][:min_len]
        
        axes[0, 0].loglog(avg_freqs[1:], avg_psd[1:], 'k-', linewidth=3, label='Average')
        axes[0, 0].set_title('Power Spectral Density: Close Prices')
        axes[0, 0].set_xlabel('Frequency (cycles/minute)')
        axes[0, 0].set_ylabel('Power Spectral Density')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Find dominant frequencies
        peak_indices = signal.find_peaks(avg_psd[1:], height=np.percentile(avg_psd[1:], 90))[0]
        dominant_freqs = avg_freqs[1:][peak_indices]
        dominant_periods = 1 / dominant_freqs  # Convert to periods in minutes
        
        # Plot dominant periods
        axes[0, 1].bar(range(len(dominant_periods)), dominant_periods, alpha=0.7)
        axes[0, 1].set_title('Dominant Periods (Minutes)')
        axes[0, 1].set_xlabel('Peak Index')
        axes[0, 1].set_ylabel('Period (minutes)')
        axes[0, 1].grid(True, alpha=0.3)
    
    # Analyze different frequency bands
    band_analysis = {}
    for band_name, period_minutes in FREQUENCY_BANDS.items():
        band_power = []
        
        for dataset in datasets[:3]:
            df = dataset.copy()
            normalized_close = normalize_price_data(df['close'])
            
            if len(normalized_close) > period_minutes * 2:
                # Resample to the target frequency
                resampled = normalized_close[::period_minutes]
                if len(resampled) > 10:
                    variance = np.var(resampled)
                    band_power.append(variance)
        
        if band_power:
            band_analysis[band_name] = np.mean(band_power)
    
    # Plot frequency band analysis
    if band_analysis:
        bands = list(band_analysis.keys())
        powers = list(band_analysis.values())
        
        axes[1, 0].bar(bands, powers, alpha=0.7)
        axes[1, 0].set_title('Power by Frequency Band')
        axes[1, 0].set_ylabel('Average Variance')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
    
    # Coherence analysis between price and volume
    sample_df = datasets[0].copy()
    normalized_close = normalize_price_data(sample_df['close'])
    normalized_volume = sample_df['volume'] / sample_df['volume'].mean()
    
    if len(normalized_close) > 100 and len(normalized_volume) > 100:
        min_len = min(len(normalized_close), len(normalized_volume))
        # Convert to numpy arrays to avoid pandas indexing issues
        close_array = np.array(normalized_close.iloc[:min_len])
        volume_array = np.array(normalized_volume.iloc[:min_len])
        
        freqs, coherence = signal.coherence(
            close_array, 
            volume_array, 
            fs=1.0
        )
        
        axes[1, 1].plot(freqs[1:], coherence[1:], 'purple', linewidth=2)
        axes[1, 1].set_title('Price-Volume Coherence')
        axes[1, 1].set_xlabel('Frequency (cycles/minute)')
        axes[1, 1].set_ylabel('Coherence')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'spectral_density_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return band_analysis

def analyze_swing_patterns(datasets):
    """Analyze swing trading patterns and optimal timeframes."""
    print("\nüìä Analyzing swing trading patterns...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Swing Trading Pattern Analysis', fontsize=16, fontweight='bold')
    
    swing_analysis = {}
    
    for timeframe_name, window_size in FREQUENCY_BANDS.items():
        if window_size > 60:  # Focus on swing timeframes (1hr+)
            continue
            
        timeframe_metrics = []
        
        for dataset in datasets:
            df = dataset.copy()
            
            # Calculate rolling statistics
            df['sma'] = df['close'].rolling(window=window_size).mean()
            df['std'] = df['close'].rolling(window=window_size).std()
            df['upper_band'] = df['sma'] + 2 * df['std']
            df['lower_band'] = df['sma'] - 2 * df['std']
            
            # Identify swing points
            df['swing_high'] = (df['high'] > df['high'].shift(1)) & (df['high'] > df['high'].shift(-1))
            df['swing_low'] = (df['low'] < df['low'].shift(1)) & (df['low'] < df['low'].shift(-1))
            
            # Calculate metrics
            swing_highs = df[df['swing_high']]['high']
            swing_lows = df[df['swing_low']]['low']
            
            if len(swing_highs) > 1 and len(swing_lows) > 1:
                avg_swing_range = np.mean(swing_highs) - np.mean(swing_lows)
                swing_frequency = (len(swing_highs) + len(swing_lows)) / len(df)
                
                timeframe_metrics.append({
                    'avg_swing_range': avg_swing_range,
                    'swing_frequency': swing_frequency,
                    'volatility': df['close'].std(),
                    'trend_strength': abs(df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]
                })
        
        if timeframe_metrics:
            swing_analysis[timeframe_name] = {
                'avg_swing_range': np.mean([m['avg_swing_range'] for m in timeframe_metrics]),
                'swing_frequency': np.mean([m['swing_frequency'] for m in timeframe_metrics]),
                'volatility': np.mean([m['volatility'] for m in timeframe_metrics]),
                'trend_strength': np.mean([m['trend_strength'] for m in timeframe_metrics])
            }
    
    # Plot swing analysis results
    if swing_analysis:
        timeframes = list(swing_analysis.keys())
        
        # Plot 1: Swing frequency
        swing_freqs = [swing_analysis[tf]['swing_frequency'] for tf in timeframes]
        axes[0, 0].bar(timeframes, swing_freqs, alpha=0.7)
        axes[0, 0].set_title('Swing Frequency by Timeframe')
        axes[0, 0].set_ylabel('Swings per Data Point')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Average swing range
        swing_ranges = [swing_analysis[tf]['avg_swing_range'] for tf in timeframes]
        axes[0, 1].bar(timeframes, swing_ranges, alpha=0.7, color='orange')
        axes[0, 1].set_title('Average Swing Range by Timeframe')
        axes[0, 1].set_ylabel('Average Range')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Volatility
        volatilities = [swing_analysis[tf]['volatility'] for tf in timeframes]
        axes[1, 0].bar(timeframes, volatilities, alpha=0.7, color='red')
        axes[1, 0].set_title('Volatility by Timeframe')
        axes[1, 0].set_ylabel('Standard Deviation')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 4: Trend strength
        trend_strengths = [swing_analysis[tf]['trend_strength'] for tf in timeframes]
        axes[1, 1].bar(timeframes, trend_strengths, alpha=0.7, color='green')
        axes[1, 1].set_title('Trend Strength by Timeframe')
        axes[1, 1].set_ylabel('Relative Price Change')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'swing_pattern_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return swing_analysis

# ==============================================================================
# TimesFM Frequency Testing
# ==============================================================================

def test_timesfm_frequencies(datasets):
    """Test different TimesFM frequency settings on sample data."""
    print("\nüß™ Testing TimesFM frequency settings...")
    
    # Note: This is a simulation since we'd need the actual TimesFM model
    # We'll analyze which frequency setting would theoretically work best
    
    frequency_recommendations = {}
    
    for dataset in datasets[:3]:  # Test on first 3 datasets
        df = dataset.copy()
        normalized_close = normalize_price_data(df['close'])
        
        if len(normalized_close) < CONTEXT_LENGTH + HORIZON_LENGTH:
            continue
        
        # Split data for testing
        context_data = normalized_close[:CONTEXT_LENGTH]
        actual_future = normalized_close[CONTEXT_LENGTH:CONTEXT_LENGTH + HORIZON_LENGTH]
        
        # Analyze characteristics that would influence frequency choice
        context_volatility = np.std(context_data)
        context_trend = (context_data.iloc[-1] - context_data.iloc[0]) / context_data.iloc[0]
        context_autocorr = np.corrcoef(context_data[:-1], context_data[1:])[0, 1]
        
        # Determine optimal frequency based on characteristics
        if context_volatility > 0.01 and abs(context_autocorr) > 0.8:
            # High volatility, high autocorr -> High frequency (0)
            recommended_freq = 0
            reason = "High volatility with strong autocorrelation"
        elif context_volatility > 0.005 and abs(context_trend) > 0.02:
            # Medium volatility, strong trend -> Medium frequency (1)
            recommended_freq = 1
            reason = "Medium volatility with strong trend"
        else:
            # Low volatility -> Low frequency (2)
            recommended_freq = 2
            reason = "Low volatility, stable pattern"
        
        frequency_recommendations[df['file_name'].iloc[0]] = {
            'recommended_freq': recommended_freq,
            'reason': reason,
            'volatility': context_volatility,
            'trend': context_trend,
            'autocorr': context_autocorr
        }
    
    # Summarize recommendations
    freq_counts = {}
    for rec in frequency_recommendations.values():
        freq = rec['recommended_freq']
        if freq not in freq_counts:
            freq_counts[freq] = []
        freq_counts[freq].append(rec)
    
    print("\nüìã TimesFM Frequency Recommendations:")
    print("=" * 50)
    
    for freq, recs in freq_counts.items():
        freq_name = {0: "High Frequency", 1: "Medium Frequency", 2: "Low Frequency"}[freq]
        print(f"\n{freq_name} (freq={freq}): {len(recs)} datasets")
        
        avg_vol = np.mean([r['volatility'] for r in recs])
        avg_trend = np.mean([r['trend'] for r in recs])
        avg_autocorr = np.mean([r['autocorr'] for r in recs])
        
        print(f"  Average volatility: {avg_vol:.6f}")
        print(f"  Average trend: {avg_trend:.6f}")
        print(f"  Average autocorr: {avg_autocorr:.6f}")
        
        print("  Reasons:")
        for r in recs:
            print(f"    - {r['reason']}")
    
    # Overall recommendation
    most_common_freq = max(freq_counts.keys(), key=lambda k: len(freq_counts[k]))
    overall_recommendation = {
        'frequency': most_common_freq,
        'frequency_name': {0: "High Frequency", 1: "Medium Frequency", 2: "Low Frequency"}[most_common_freq],
        'percentage': len(freq_counts[most_common_freq]) / len(frequency_recommendations) * 100,
        'sample_size': len(frequency_recommendations)
    }
    
    return frequency_recommendations, overall_recommendation

# ==============================================================================
# Main Analysis Function
# ==============================================================================

def main():
    """Run comprehensive frequency analysis."""
    print("üöÄ ES Dataset Frequency Analysis for TimesFM Optimization")
    print("=" * 60)
    
    try:
        # Load datasets
        datasets = load_random_es_files(SAMPLE_SIZE)
        if not datasets:
            print("‚ùå No datasets loaded. Exiting.")
            return
        
        print(f"\nüìä Analyzing {len(datasets)} datasets...")
        print(f"üìè Context length: {CONTEXT_LENGTH} minutes")
        print(f"üîÆ Prediction horizon: {HORIZON_LENGTH} minutes")
        
        # Run analyses
        print("\n" + "="*60)
        
        # 1. Intraday volatility analysis
        hourly_vol, session_vol = analyze_intraday_volatility(datasets)
        
        # 2. Autocorrelation analysis
        autocorr_results = analyze_autocorrelation(datasets)
        
        # 3. Spectral density analysis
        spectral_results = analyze_spectral_density(datasets)
        
        # 4. Swing pattern analysis
        swing_results = analyze_swing_patterns(datasets)
        
        # 5. TimesFM frequency testing
        freq_recs, overall_rec = test_timesfm_frequencies(datasets)
        
        # Generate final recommendations
        print("\n" + "="*60)
        print("üéØ FINAL RECOMMENDATIONS")
        print("="*60)
        
        print(f"\nüîß Optimal TimesFM Frequency Setting:")
        print(f"   Frequency: {overall_rec['frequency']} ({overall_rec['frequency_name']})")
        print(f"   Confidence: {overall_rec['percentage']:.1f}% of samples")
        print(f"   Sample size: {overall_rec['sample_size']} datasets")
        
        # Market session insights
        if 'Regular Hours' in session_vol:
            peak_session = session_vol.idxmax()
            print(f"\nüìà Peak Volatility Session: {peak_session}")
            print(f"   Volatility: {session_vol[peak_session]:.6f}")
        
        # Spectral insights
        if spectral_results:
            dominant_band = max(spectral_results.keys(), key=lambda k: spectral_results[k])
            print(f"\nüåä Dominant Frequency Band: {dominant_band}")
            print(f"   Power: {spectral_results[dominant_band]:.6f}")
        
        # Practical recommendations
        print(f"\nüí° Practical Recommendations:")
        print(f"   ‚Ä¢ Use frequency={overall_rec['frequency']} for TimesFM predictions")
        print(f"   ‚Ä¢ Current context length ({CONTEXT_LENGTH} min) is appropriate")
        print(f"   ‚Ä¢ Current horizon ({HORIZON_LENGTH} min) aligns with intraday patterns")
        print(f"   ‚Ä¢ Consider higher frequency during market open/close")
        print(f"   ‚Ä¢ Monitor model performance during high volatility periods")
        
        print(f"\nüìÅ Analysis plots saved to: {OUTPUT_DIR}")
        print("‚úÖ Frequency analysis complete!")
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
